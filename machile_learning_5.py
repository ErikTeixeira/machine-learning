# -*- coding: utf-8 -*-
"""machile learning - 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Opz2GYMkM6vjkuvcjHNHKLhCrMSTWSjh

modelos de regressão, que têm intuito de prever valores numéricos, quanto para modelos de classificação, que têm intuito de prever categorias.

</br>
três modelos: o modelo KNN, o modelo Bernoulli Naive Bayes e o modelo árvore de decisão

# Classificação e Regressão

## Classificação
Quando precisamos prever a qual categoria pertence uma determinada amostra, trata-se de um problema de classificação

### Ex:
* Prever se um(a) determinado(a) paciente está com Covid.
* Se um(a) cliente está propenso(a) a desistir da compra.
* Se algum(a) usuário(a) web está propenso(a) a clicar em um anúncio.


`Nesses casos mencionados, a previsão se concentra em 0 ou 1 `(Covid/não Covid, desistir/não desistir, clicar/não clicar) que é denominada de `classificação binária`, na qual existem somente duas classes. Há também casos em que a classificação se dá com mais duas classes, chamada de `classificação multiclasse`, como a filtragem dos e-mails em “principal”, “social”, “promoções”, “importantes” ou “fóruns”.

# Regressão
Quando precisamos prever um valor numérico específico, isso indica que estamos lidando com um problema de regressão.

## Ex:
* preços/custos futuros;
* estoque;
* receita futura.

Nessas situações, podemos utilizar algum modelo de regressão para realizar essas previsões e apresentar como resposta algum valor contínuo relacionado ao problema.

## Tipos de algoritmos
* Linear Regression;
* Random Forest Regressor;
* Support Vector Regression (SVR).

# Divisão dos dados em treino e teste

### Dados de treino
Os dados de treino são aqueles utilizados para a criação e treinamento do modelo. Normalmente a maioria dos dados, cerca de 70%, são utilizados para treinamento.

### Dados de teste
Os dados de teste são utilizados para comprovar que o modelo realmente funciona. Eles não são utilizados no treinamento do modelo e normalmente representam 30% da totalidade dos dados.


---


## No momento de realizar a separação desses dados é importante que ela seja feita de forma aleatória, para garantirmos que não haverá nenhum padrão no momento de divisão dos dados
"""

# realizar o balanceamento dos dados, utilizaremos uma técnica chamada de Oversampling, precisa do imbalanced-learn

!pip install -U imbalanced-learn

"""# Instalar o CSV - Customer-Churn.csv
### Está neste link
https://github.com/alura-cursos/ML_Classificacao_por_tras_dos_panos/tree/main/Dados
"""

import pandas as pd

dados = pd.read_csv("/content/Customer-Churn.csv")

# mostra linhas e colunas
dados.shape

dados.head()

"""## Variável Categórica
### Agrupamento por características em comun
### Agrupamento por valores qualitativos
###### ex: conjuge, dependentes, maior 65 anos 1 é maior e 0 não é maior

## Varíavel Numérica
### Medida ou conjunto infinito de valores
###### ex: meses de contrato, conta mensal
"""

# transformar os falores em números para a maquina

# forma manual
traducao_dic = {
    "Sim" : 1,
    "Nao" : 0
}

dadosmodificados = dados[["Conjuge", "Dependentes", "TelefoneFixo", "PagamentoOnline", "Churn"]].replace(traducao_dic)

dadosmodificados.head()

# transformação pelo get_dummies  -  função do pandas
# o dummies está transformando em 1 e 0 as colunas que faltaram, drop remove as que já foram modificadas
dummie_dados = pd.get_dummies(dados.drop(["Conjuge", "Dependentes", "TelefoneFixo", "PagamentoOnline", "Churn"], axis = 1))

# junção dos dados transformados com o restante - juntando o dadosmodificados com o dummie_dados  - juntando por meio das colunas
dados_final = pd.concat([dadosmodificados, dummie_dados], axis = 1)
dados_final.head()

"""
# X = inputs (dados de entrada) - caracteristicas do cliente
# Y = output (dados de saída) - churn
"""

# conseguir ver as 39 colunas que se formaram

pd.set_option("display.max_columns", 39)

dados_final.head()

"""### Y é o resultado de uma função desconhecida que a máquina irá aprender com o algoritmo e aplicar aos nossos dados X. Y é resultado dessa função desconhecida aplicada ao X

## Yi = f(Xi)
"""

# dados maria
# Conjuge	Dependentes	TelefoneFixo	PagamentoOnline	Churn	Maior65Anos	MesesDeContrato	ContaMensal	VariasLinhasTelefonicas_Nao	VariasLinhasTelefonicas_SemServicoTelefonico	VariasLinhasTelefonicas_Sim	ServicoDeInternet_DSL	ServicoDeInternet_FibraOptica	ServicoDeInternet_Nao	SegurancaOnline_Nao	SegurancaOnline_SemServicoDeInternet	SegurancaOnline_Sim	BackupOnline_Nao	BackupOnline_SemServicoDeInternet	BackupOnline_Sim	SeguroNoDispositivo_Nao	SeguroNoDispositivo_SemServicoDeInternet	SeguroNoDispositivo_Sim	SuporteTecnico_Nao	SuporteTecnico_SemServicoDeInternet	SuporteTecnico_Sim	TVaCabo_Nao	TVaCabo_SemServicoDeInternet	TVaCabo_Sim	StreamingDeFilmes_Nao	StreamingDeFilmes_SemServicoDeInternet	StreamingDeFilmes_Sim	TipoDeContrato_DoisAnos	TipoDeContrato_Mensalmente	TipoDeContrato_UmAno	FormaDePagamento_CartaoDeCredito	FormaDePagamento_ChequeDigital	FormaDePagamento_ChequePapel	FormaDePagamento_DebitoEmConta
Xmaria = [[0,0,1,1,0,0,39.90,1,0,0,0,1,0,1,0,0,0,0,1,1,1,0,0,1,0,1,0,0,0,0,1,0,0,1,0,0,0,1]]

# Commented out IPython magic to ensure Python compatibility.
# verificar se os dados estão desbalanceados

import seaborn as sns
# %matplotlib inline

ax = sns.countplot(x='Churn', data=dados_final)

"""### Ao analisarmos a variável classificadora ‘Churn’, podemos notar que `há menos clientes contendo Sim - 1 do que Não - 0`. Para que o aprendizado do algoritmo não seja afetado pela falta de informações referentes ao Churn de clientes que nos deixaram (Sim), `é necessário aplicar uma técnica de balanceamento`"""

# Para podermos aplicar o SMOTE, devemos separar  os dados em variáveis características e resposta

# Divisão de dados  -  inputs e outputs    - x são todas as colunas(caracteristicas), monos a churn
X = dados_final.drop('Churn', axis = 1)
y = dados_final['Churn']


from imblearn.over_sampling import SMOTE

smt = SMOTE(random_state=123)  # Instancia um objeto da classe SMOTE
X, y = smt.fit_resample(X, y)  # Realiza a reamostragem do conjunto de dados


dados_final = pd.concat([X, y], axis=1)  # Concatena a variável target (y) com as features (X)

# Verifica se o balanceamento e a concatenação estão corretos.
dados_final.head(2)

ax = sns.countplot(x='Churn', data=dados_final)  # plotando a variável target balanceada.

"""## k-nearest neighbors (KNN)"""

# Função desconhecida  -  método KNK  ou k-vizinhos mais próximos

# Algoritmo de machile learning supervisionado,  usado em tarefas de classificação ou regressão

# 1 - receber as informações  - de um usuario menos o churn(y)
# 2 - calcular as distâncias  - pega as informações do usuario e calcula a distância com cada um dos clientes
# 3 - ordenar da maior distância para a maior
# 4 - fazer uma contagem das classes  - quantas classes aparecem em cada classe ordenada
# 5 - classificar a partir dos K - vizinhos
# 6 - defini o churn(y)


# Classificação interferida pelo valor de K

# K - se for muito pequeno  -  OverFitting
#    Vai se ajustar perfeitamente aos dados de entrada, ao X dos nossos clientes. E quando chegar essa informação da Maria, esse dado novo não vai conseguir classificá-la de forma correta porque ele só vai conseguir classificar valores iguais a aqueles nossos dados que usou para treiná-lo

# K - se fro muito grande - UnderFitting
#    O nosso modelo vai ver um número muito grande de vizinhos, muitas classes e não vai conseguir identificar a classe correta de cada variável, de cada cliente. Ele vai classificar de forma errada também


# Métrica de distância
# - importante testar diferentes métricas tmb

Xmaria
# Ymaria = ?

# Divisão de dados  -  inputs e outputs    - x são todas as colunas(caracteristicas), monos a churn
X = dados_final.drop('Churn', axis = 1)
y = dados_final['Churn']


# Biblioteca para PADRONIZAR os dados     -  todos os valores tem que ter a mesma escala para calcular a distância
from sklearn.preprocessing import StandardScaler


norm = StandardScaler()

X_normalizado = norm.fit_transform(X)
X_normalizado

# Normalizar a maria

# função só é aplicada quando o nosso conjunto de dados está em formato de duas dimensões  - informações formadas de linhas e colunas
# Maria está um vetor de características  -  transformar a maria em linhas e colunas
Xmaria_normalizado = norm.transform( pd.DataFrame(Xmaria, columns = X.columns ))

Xmaria_normalizado

"""## Métrica Euclidiana"""

# Métrica de distância  -  Euclidiana
# Subtrair as coordenadas de uma observação pela outra observação, elevar ao quadrado os resultados, somar todos os valores e extrair a raiz quadrada

# facilitar as contas
import numpy as np

a = Xmaria_normalizado

# cliente 0
b = X_normalizado[0]

# 1 - começa subtraindo
a - b

# 2 - realiza a exponenciação
np.square(a-b)

# 3 - soma
np.sum(np.square(a-b))

# 4 - fazer a raiz que vai ser a distância
np.sqrt(91.7112036526817)

"""## Funções de distância

#### Podemos modificar a medida de distância utilizando o argumento metric da função sklearn.neighbors.KNeighborsClassifier. O parâmetro pode receber os seguintes valores:

* “euclidean” para a distância euclidiana;
* “manhattan” para a distância de Manhattan;
* “minkowski” para a distância de Minkowski;
* “chebyshev” para a distância de Chebyshev.
"""

# IMPLEMENTAR O MODELO

# dividir os dados de novo
# 1 - divide em X e Y   - dados de entrada e saída
# 2 - separar treino e teste   - para poder treinar usando o knn e depois testar se funcionou


# biblioteca para divisão dos dados
from sklearn.model_selection import train_test_split

X_treino, X_teste, y_treino, y_teste = train_test_split( X_normalizado, y, test_size = 0.3, random_state = 123 )

# TREINO E TESTE

# biblioteca para criarmos o modelo de machine learning
from sklearn.neighbors import KNeighborsClassifier

# instanciar o modelo ( criar o modelo )  -  por padrão são 5 modelos  - da para mudar
knn = KNeighborsClassifier( metric = "euclidean" )

# treinando o modelo com ps dados de treino
knn.fit( X_treino, y_treino )

# testando o modelo com dados de teste
predito_knn = knn.predict( X_teste )

predito_knn

"""# Princípio da independência condicional
### Teorema de Naive Bayes    -    modele Bernoulli Naïve Bayes

#### - Utilizado em aprendizado de maquinas supervisionado (classificação)
#### - Funciona como um classificador, pois é utilizado para prever a qual categoria pertence uma determinada amostra
#### - Naïve Bayes aplica o conceito de probabilidade utilizando o teorema de Bayes

#### - O teorema de Bayes é uma fórmula utilizada para calcular a probabilidade de um evento ocorrer sabendo que um outro evento, chamado de condicionante, já ocorreu, denominado `probabilidade condicional`

## fórmula do teorema: "P(y/x) = P(X/y) * P(y)/P(X)"
* P(B|A): probabilidade de B acontecer dado que A já aconteceu;
* P(A): probabilidade de A acontecer;
* P(B): probabilidade de B acontecer.



---


Os modelos classificadores de Naïve Bayes realizam seus cálculos com base em probabilidades e desconsideram a correlação entre as variáveis.
## São os 3 modelos mais conhecidos que são implementados com o algoritmo Naïve Bayes:
* BernoulliNB()
* GaussianNB()
* MultinomialNB()
"""

# Treinar e testar

# mediana - valor central dos nossos dados ordenados
np.median(X_treino)

# resultado mediana/valor central   ->    -0.4461759755508453

# biblioteca para criar o modelo de machine learning
from sklearn.naive_bayes import BernoulliNB

# instanciar/criar o modelo   -  binarize  definir o limite para poder transformar aquela variavel em binario ou não
# colocar o valor da mediana no binarize  -  que acima desse valor vai ser 1 e abaixo 0
bnb = BernoulliNB( binarize=0.44)

# treinar
bnb.fit( X_treino, y_treino )

# testando o modelo com dados de teste
predito_bnb = bnb.predict( X_teste )

predito_bnb

"""# Árvore de decisão | Decision tree
### Pode ser usado em tarefas de classificação e regressão


---

                      Nó raiz
             true    /         \   false
          Nó de decisão      Nó de decisão | Nó pai
                  /       true  /     \   false
            Nó folha      Nó folha    Nó folha | Nó filho

### Desvantagens da árvore de decisão:
* #### Overfitting -  a árvore vai crescendo tanto que ela vai se ajustar perfeitamente àqueles dados de treinos. E ao surgir os novos dados, a árvore não vai conseguir tomar decisões para os dados novos porque ele foi ajustado perfeitamente aos nossos dados de treino
* #### Instabilidade da árvore de decisão - qualquer mudança que for feita em qualquer parte da árvore, ela vai gerar uma árvore nova

### Principais critérios de divisão dos nós:
* **Índice Gini**    -  mais utilizado
* Qui-Quadrado
*  **Ganho de informação (entropia)**   -  mais utilizado
* Redução na variância.

## No índice Gini, a ideia é medir o quão impuro está o dado, o quão heterogêneo ele está. </br>
#### Quanto maior o resultado mais heterogêneo o dado está, mais difícil de extrair informações, conseguir dividir este dado em diferentes classes </br>
#### **Agora, quanto menor mais homogêneo que seria o ideal** </br></br>
## A entropia mede a ordem dos dados  - usado como critério para verificar a impureza também </br>
#### Foca no ganho de informação </br>
#### Quanto maior a entropia mais desordenado está o dado e quanto menor **mais ordenado** está, **maior ganho de informação**

### `O Índice Gini e a entropia é feita de forma recursiva, aplicando em todos os atributos`
"""

# biblioteca para decision tree

from sklearn.tree import DecisionTreeClassifier

# instanciar o modelo
dtc = DecisionTreeClassifier( criterion = "entropy", random_state = 42 )

# treinar o modelo
dtc.fit( X_treino, y_treino )

# verificar a importancia de cada atributo
dtc.feature_importances_

# quanto menor o valor mais homogeneo é, maior o ganho de informação

predito_arvore = dtc.predict( X_teste )

predito_arvore

"""## Validação dos modelo

### Matriz de confusão
Uma tabela contendo quatro formas de visualizar os resultados

                      PREDITO
                  SIM   |   NÃO
      REAL   SIM   VP   |   FN
      REAL   NÃO   FP   |   VN


* VP - verdadeiro positivo  - valor correto
* FN - falso negativo  - valor incorreto  - deveria ser um sim, mas foi predito com não
* FP - falso positivo  - y previsto com sim, mas o valor real é não
* VN - verdadeiro negativo  - valor correto - é um não e foi previsto com não

#  A matriz de confusão pode se apresentar também na ordem inversa
"""

# biblioteca para matriz
from sklearn.metrics import confusion_matrix

# knn
# y_teste - valor real, predito_knn - valor predito pelo nosso modelo
print(confusion_matrix( y_teste, predito_knn ))

#   VP     FN             1242 - valores positivos certos         327  - valores positivos errados
# [[1242  327]
#   FP    VN             248 - valores incorretos errados         1288  - valores incorretos certos
# [ 248 1288]]


# bnb
print("\n", confusion_matrix( y_teste, predito_bnb ))

# arvore de decisao
print("\n", confusion_matrix( y_teste, predito_arvore ))

"""## Métrica Acuracia
#### Mede quanto nosso modelo acertou do total

ACC = TP + TN / TP + FP + TN + FN


---
A acurácia é uma boa indicação geral da performance do modelo. Mas, em algumas situações, como em modelos de identificação de fraudes, ela pode ser um pouco enganosa
"""

# biblioteca
from sklearn.metrics import accuracy_score

# modelo KNN
print( accuracy_score( y_teste, predito_knn ) )

# depois pode multiplicar por 100 para mostrar melhor que é em porcentagem

# modelo BNB
print( accuracy_score( y_teste, predito_bnb ) )

# modelo arvore de decisao
print( accuracy_score( y_teste, predito_arvore ) )

"""## Métrica precisão

Calcula quantos verdadeiros positivos foram classificados de forma correta

PS = TP / TP + TF

---
A precisão pode ser utilizada em situações em que os Falsos Positivos são mais prejudiciais que os Falsos Negativos. Por exemplo, em um modelo de classificação de um bom investimento
"""

# biblioteca
from sklearn.metrics import precision_score

# modelo KNN
print( precision_score( y_teste, predito_knn ) )

# modelo BNB
print( precision_score( y_teste, predito_bnb ) )

# modelo arvore de decisao
print( precision_score( y_teste, predito_arvore ) )

"""## Métrica recall

Mede quão bom nosso modelo está em classificar os resultados realmentes positivos

RC = TP / TP + FN
"""

# biblioteca
from sklearn.metrics import recall_score

# modelo KNN
print(recall_score( y_teste, predito_knn ))

# modelo BNB
print(recall_score( y_teste, predito_bnb ))

# modelo arvore de decisao
print(recall_score( y_teste, predito_arvore ))

"""## Modelo escolhi - Precisão, mas poderia ser a recall

#### Escolher a métrica depende dos dados que está trabalhando, do problema que quer solucionar
"""

# analises das precisoes calculadas   -  * 100 para ser em porcentagem
print(f"Modelo KNN: {precision_score(y_teste, predito_knn)* 100 :.2f} %")
print(f"Modelo Bernoulli de Naive Bayes:' {precision_score( y_teste, predito_bnb )* 100 :.2f} % ")
print(f"Modelo Àrvore de Decisão:' {precision_score( y_teste, predito_arvore )* 100 :.2f} % ")